{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Optuna\n",
    "import optuna\n",
    "\n",
    "# External libraries\n",
    "import numpy\n",
    "from tensorflow.python.keras.datasets import mnist\n",
    "from tensorflow.python.keras.layers import (Dense,\n",
    "                                            Flatten,\n",
    "                                            Input)\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== MNIST dataset ==========================\n",
      "Train data shape   : (60000, 28, 28)\n",
      "Train labels shape : (60000, 10)\n",
      "Test data shape    : (10000, 28, 28)\n",
      "Test labels shape  : (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve MNIST train/test images\n",
    "((x_train, y_train), \n",
    " (x_test, y_test)) = mnist.load_data()\n",
    "\n",
    "# Normalize MNIST images\n",
    "x_train = x_train.astype(numpy.float32) / 255 - 0.5\n",
    "x_test = x_test.astype(numpy.float32) / 255 - 0.5\n",
    "\n",
    "# Use 1-hot encoding on train and test labels\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Report train and test dataset properties\n",
    "print('== MNIST dataset ==========================')\n",
    "print(f'Train data shape   : {x_train.shape}')\n",
    "print(f'Train labels shape : {y_train.shape}')\n",
    "print(f'Test data shape    : {x_test.shape}')\n",
    "print(f'Test labels shape  : {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(dense_layer_sizes,\n",
    "                dense_layer_activations,\n",
    "                optimizer):\n",
    "    # Define input data\n",
    "    input_layer = Input(shape=(28, \n",
    "                               28),\n",
    "                        dtype=numpy.float32)\n",
    "    \n",
    "    # Define model topology\n",
    "    flatten = Flatten()(input_layer)\n",
    "    dense_layers = list()\n",
    "    for i, (units, activation) in enumerate(zip(dense_layer_sizes,\n",
    "                                                dense_layer_activations)):\n",
    "        if i == 0:\n",
    "            dense_layers.append(Dense(units=units,\n",
    "                                      activation=activation)(flatten))\n",
    "        else:\n",
    "            dense_layers.append(Dense(units=units,\n",
    "                                      activation=activation)(dense_layers[-1]))\n",
    "    output_layer = Dense(units=10,\n",
    "                         activation='sigmoid')(dense_layers[-1])\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = Model(inputs=input_layer,\n",
    "                  outputs=output_layer)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  metrics=['accuracy'],\n",
    "                  loss='categorical_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def test_model(model,\n",
    "               batch_size,\n",
    "               epochs,\n",
    "               verbose):\n",
    "    train_stats = model.fit(x=x_train,\n",
    "                            y=y_train,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=verbose).history\n",
    "    (loss,\n",
    "     accuracy) = model.evaluate(x=x_test,\n",
    "                                y=y_test,\n",
    "                                verbose=verbose)\n",
    "    \n",
    "    return (train_stats,\n",
    "            {'loss': loss,\n",
    "             'accuracy': accuracy})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize train parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2019-09-26 16:57:38,318]\u001b[0m Finished trial#0 resulted in value: 0.32899999618530273. Current best value is 0.32899999618530273 with parameters: {'optimizer': 'sgd', 'batch_size': 152, 'epochs': 34}.\u001b[0m\n",
      "\u001b[32m[I 2019-09-26 16:58:23,064]\u001b[0m Finished trial#1 resulted in value: 0.3400000035762787. Current best value is 0.3400000035762787 with parameters: {'optimizer': 'sgd', 'batch_size': 199, 'epochs': 43}.\u001b[0m\n",
      "\u001b[32m[I 2019-09-26 16:59:14,376]\u001b[0m Finished trial#2 resulted in value: 0.8371999859809875. Current best value is 0.8371999859809875 with parameters: {'optimizer': 'adam', 'batch_size': 251, 'epochs': 46}.\u001b[0m\n",
      "\u001b[32m[I 2019-09-26 16:59:59,356]\u001b[0m Finished trial#3 resulted in value: 0.22040000557899475. Current best value is 0.8371999859809875 with parameters: {'optimizer': 'adam', 'batch_size': 251, 'epochs': 46}.\u001b[0m\n",
      "\u001b[32m[I 2019-09-26 17:01:35,004]\u001b[0m Finished trial#4 resulted in value: 0.4927999973297119. Current best value is 0.8371999859809875 with parameters: {'optimizer': 'adam', 'batch_size': 251, 'epochs': 46}.\u001b[0m\n",
      "\u001b[32m[I 2019-09-26 17:01:57,645]\u001b[0m Finished trial#5 resulted in value: 0.6355999708175659. Current best value is 0.8371999859809875 with parameters: {'optimizer': 'adam', 'batch_size': 251, 'epochs': 46}.\u001b[0m\n",
      "\u001b[32m[I 2019-09-26 17:03:04,485]\u001b[0m Finished trial#6 resulted in value: 0.9133999943733215. Current best value is 0.9133999943733215 with parameters: {'optimizer': 'adam', 'batch_size': 72, 'epochs': 23}.\u001b[0m\n",
      "\u001b[32m[I 2019-09-26 17:03:24,243]\u001b[0m Finished trial#7 resulted in value: 0.3264999985694885. Current best value is 0.9133999943733215 with parameters: {'optimizer': 'adam', 'batch_size': 72, 'epochs': 23}.\u001b[0m\n",
      "\u001b[32m[I 2019-09-26 17:05:29,940]\u001b[0m Finished trial#8 resulted in value: 0.9240000247955322. Current best value is 0.9240000247955322 with parameters: {'optimizer': 'adam', 'batch_size': 77, 'epochs': 46}.\u001b[0m\n",
      "\u001b[32m[I 2019-09-26 17:05:58,966]\u001b[0m Finished trial#9 resulted in value: 0.3116999864578247. Current best value is 0.9240000247955322 with parameters: {'optimizer': 'adam', 'batch_size': 77, 'epochs': 46}.\u001b[0m\n",
      "\u001b[32m[I 2019-09-26 17:08:53,473]\u001b[0m Finished trial#10 resulted in value: 0.9330999851226807. Current best value is 0.9330999851226807 with parameters: {'optimizer': 'adam', 'batch_size': 36, 'epochs': 36}.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Guess hyperparameter values\n",
    "    optimizer = trial.suggest_categorical('optimizer',\n",
    "                                          ['adam',\n",
    "                                           'sgd'])\n",
    "    batch_size = trial.suggest_int('batch_size',\n",
    "                                   32,\n",
    "                                   256)\n",
    "    epochs = trial.suggest_int('epochs',\n",
    "                               5,\n",
    "                               50)\n",
    "    \n",
    "    # Initialize and test model\n",
    "    model = build_model([16],\n",
    "                        ['softmax'],\n",
    "                        optimizer)\n",
    "    (train_stats,\n",
    "     test_stats) = test_model(model,\n",
    "                              batch_size,\n",
    "                              epochs,\n",
    "                              False)\n",
    "    \n",
    "    return test_stats['accuracy']\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective,\n",
    "               timeout=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "{'optimizer': 'adam', 'batch_size': 36, 'epochs': 36}\n",
      "0.9330999851226807\n"
     ]
    }
   ],
   "source": [
    "print(len(study.trials))\n",
    "print(study.best_params)\n",
    "print(study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
